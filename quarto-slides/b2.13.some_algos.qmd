---
title: Подходы к проектированию алгоритмов
---

# Распараллеливание

## Основы

* Никакие алгоритмы не помогут ускорить процесс при помощи нескольких процессоров с такой же суммарной вычислительной мощностью. Только замедлят.
* Разбиение данных с параллельной обработкой фрагментов и разбиение алгоритмов с конвейеризацией --- наиболее универсальные подходы.

Основная проблема в том, что поднять мощность одного процессора в n раз очень непросто.

## Уровни параллелизма

::: {.incremental}
* Уровень заданий.
  Несколько независимых заданий одновременно выполняются на
  разных процессорах, практически не взаимодействуя друг с другом.
  Реализуется на ВС с множеством процессоров в многозадачном
  режиме.
* Уровень программ.
  Части одной задачи выполняются на множестве процессоров. Достигается на параллельных ВС.
* Уровень команд.
  Выполнение команды разделяется на фазы, а фазы нескольких
  последовательных команд м.б. перекрыты за счет конвейеризации.
  Достижим на ВС с одним процессором.
* Уровень битов и скаляров (арифметический уровень)
  Биты слова обрабатываются одновременно. Реализуется в обычных и суперскалярных процессорах.
  Скаляры параллельно обрабатываются в векторных процессорах.
:::

## Типы архитектур

* Суперскалярные процессоры
* VLIW
* Векторные, матричные и тензорные процессоры
* Кластеры, гриды

## Языки программирования

* Обычные
* С использованием библиотек параллельной обработки данных
* С использованием параллельных расширений
* Специализированные параллельные языки
* Обычные языки с естественной поддержкой параллельности

## Топологии

* Шина
* Кольцо
* Плоскость
* Цилиндр
* Тор
* Матрица (полный граф)
* M динамических линий между N процессорами
* Гиперкуб
* Топологии, имитирующие физику задач

## Показатели эффективности распараллеливания

* $O(n), O(1)$ --- количество операций на системе с n или 1 процессорами
* $T(n), T(1)$ --- время выполнения
* $S(n) = \frac{T(1)}{T(n)}$ --- ускорение

. . .

Берём $T(1) = O(1)$

Параллельное выполнение д.б., по крайней мере, не медленнее, чем послдовательное, так что:

* $T(n) \le O(n) \le n O(1) = n T(1)$
* $1 \le S(n) \le n$ --- ускорение

. . .

* $1/n \le E(n) = S(n)/n = \frac{T(1)}{nT(n)} \le 1$ --- эффективность


## Закон Амдала

$f$ --- доля последовательного кода, $(1-f)$ --- параллельного.
	
$$T_{par} = f T_{seq} + \frac{(1-f)T_{seq}}{n}$$
$\Rightarrow$
$$S = T_{seq}/T_{par} = \frac{n}{1 + (n-1)f} ~ {\stackrel{{}~ n \rightarrow \infty ~{}}{\longrightarrow}} ~ 1/f$$


# Примеры параллельных алгоритмов

## Алгоритм Штрассена
$${C} = {A} {B} \qquad {A},{B},{C} \in \mathbb{R}^{2^n \times 2^n}$$

$$
{A} =
\begin{bmatrix}
{A}_{1,1} & {A}_{1,2} \\
{A}_{2,1} & {A}_{2,2}
\end{bmatrix},
{B} =
\begin{bmatrix}
{B}_{1,1} & {B}_{1,2} \\
{B}_{2,1} & {B}_{2,2}
\end{bmatrix},
{C} =
\begin{bmatrix}
{C}_{1,1} & {C}_{1,2} \\
{C}_{2,1} & {C}_{2,2}
\end{bmatrix}.
$$

$${A}_{i,j}, {B}_{i,j}, {C}_{i,j} \in \mathbb{R}^{2^{n-1} \times 2^{n-1}}$$

. . .

Это 8 умножений:

$$
{C}_{1,1} = {A}_{1,1} {B}_{1,1} + {A}_{1,2} {B}_{2,1};
{C}_{1,2} = {A}_{1,1} {B}_{1,2} + {A}_{1,2} {B}_{2,2};
$$ $$
{C}_{2,1} = {A}_{2,1} {B}_{1,1} + {A}_{2,2} {B}_{2,1};
{C}_{2,2} = {A}_{2,1} {B}_{1,2} + {A}_{2,2} {B}_{2,2}.
$$

[А можно за 7!](http://en.wikipedia.org/wiki/Strassen_algorithm)

## Поразрядная сортировка
Пример для двоичной системы.

* LSD -- сперва младшие, затем консервативно (стабильно) относительно порядка младших старшие:
  * Сперва фильтруются те, у которых старший бит 0,
  * затем те, у кого 1,
  * затем они сливаются.

  . . .

  * Это частный случай блочной (корзинной, карманной) сортировки

* MSD -- сперва старшие, затем рекурсивно младшие:
  * Фактически, это QuickSort, но разделение по значению очередного разряда

  . . .

* И старшие с младшими можно сортировать параллельно!

## BucketSort
Развитие идеи QuickSort

* Распределяем по блокам

![](img2/13.Bucket_sort_1.png){height=20%}
  
. . .

* Сортируем блоки (можно параллельно и распределённо)

![](img2/13.Bucket_sort_2.png){height=20%}

. . .

* Сливаем

. . .

Как и QuickSort, можно затормозить специально подобранными данными.

## Аппаратная реализация

[Раскладочно-подборочные машины](https://ru.wikipedia.org/wiki/%D0%A0%D0%B0%D1%81%D0%BA%D0%BB%D0%B0%D0%B4%D0%BE%D1%87%D0%BD%D0%BE-%D0%BF%D0%BE%D0%B4%D0%B1%D0%BE%D1%80%D0%BE%D1%87%D0%BD%D1%8B%D0%B5_%D0%BC%D0%B0%D1%88%D0%B8%D0%BD%D1%8B)

## Map-Reduce: модель

Парадигма и паттерн. Но не «технология».

. . .

$V$ -- пространство значений

$m : V \rightarrow C$ -- функция разбиения на классы эквивалентности

$r : W \in 2^V \rightarrow R ~ | ~ \forall v \in W ~ f(v) = c \in C$ -- функция редукции
	
. . .
	
Редукция может быть иерархической

	
## Map-Reduce: топология

![](img2/13.mrfigure.png){height=80%}


## Map-Reduce: некомпьютерный пример

* Несколько человек берут стаканы с деньгами, выбирают монетки и сортируют на кучки с одним достоинством (map).
* Считается количество в каждой кучке и, таким образом, её достоинство (reduce1).
* Суммируются достоинства кучек монет: сперва для одного человека (reduce2), потом общее (reduce3).

# Жадное и динамическое программирование

## Суть жадных алгоритмов

* Принимать решение, дающее максимальную выгоду на текущем шаге
* Бери, пока (и что) дают

. . .

Не претендуют на поиск оптимального решения, но для каких-то задач находят его

## Жадный алгоритм на базе металлоприёмки

1. На складе мелкий лом
2. Можем унести ограниченное количество
3. Берём самый дорогой, сколько есть и сколько влезет
4. Если осталось место, то повторяем (3) для следующего по удельной цене

## Динамическое программирование

* Решаем задачу в несколько шагов, «ветвясь в ширину»
* На следующем шаге пользуемся частичными данными со всех ветвей предыдущих шагов (как правило одного предыдущего шага)
  * важно: на каждом шаге мы должны обрабатывать лишь наилучшие промежуточные результаты, полученные на предыдущем, в этом и выгода
	
## ДП: Метод ветвей и границ / Задача о рюкзаке

[На складе не лом, а изделия](https://docs.google.com/spreadsheets/d/1fx94gkwm1B6_jQJQ9GUOxXtmDWipI8E5dN6YuX4HkUU/edit?usp=sharing) (стоят дороже)
	
![](img2/13.branches.pdf){width=100%}

## ДП: Метод ветвей и границ / Выключка абзаца

Надо минимизировать штраф за переносы подряд, слишком узкие или широкие апроши и расстояния между словами, слишком короткую последнюю строку и т.д.

[Вот как например здесь](https://cs.stackexchange.com/questions/150329/text-justification-problem-of-introduction-to-algorithm)


## ДП: Кратчайший путь
	
Кратчайший путь в графе между двумя точками.
	
* Идём «фронтом» от конечной.
* В каждой доступной вершине перезаписываем расстояние, если оно станет меньше, и помечаем, откуда пришли.
* Пока не дойдём до начальной.
	
. . .

* Каждый раз используется весь фронт (и только он)
	
* «Расстояние» в самом общем смысле. Например для ракеты взвешенная сумма расхода топлива, времени полёта и риска быть сбитой в данной точке.

# Теорема о рекуррентных соотношениях

## О чём речь?

* Задача на $n$ элементах решается за $T(n)$ (не путать с параллельным программированием)
* Есть возможность разбить её на $a$ подзадач, каждая для $\frac{n}{b}$ элементов, тогда $T(n) = aT(\frac{n}{b}) + f(n)$, где $f(n)$ --- накладные расходы на создание подзадач и слияние результатов

*Jon Louis Bentley, Dorothea Haken, and James B. Saxe. 1980. A general method for solving divide-and-conquer recurrences. SIGACT News 12, 3 (Fall 1980), 36–44. https://doi.org/10.1145/1008861.1008865*

## Вспоминаем $O$, $\Theta$ и $\Omega$

И смотрим для этого
[русскую](https://ru.wikipedia.org/wiki/%C2%ABO%C2%BB_%D0%B1%D0%BE%D0%BB%D1%8C%D1%88%D0%BE%D0%B5_%D0%B8_%C2%ABo%C2%BB_%D0%BC%D0%B0%D0%BB%D0%BE%D0%B5#%D0%94%D1%80%D1%83%D0%B3%D0%B8%D0%B5_%D0%BF%D0%BE%D0%B4%D0%BE%D0%B1%D0%BD%D1%8B%D0%B5_%D0%BE%D0%B1%D0%BE%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%B8%D1%8F),
[английскую](https://en.wikipedia.org/wiki/Big_O_notation#Family_of_Bachmann%E2%80%93Landau_notations) и
даже [немецкую](https://de.wikipedia.org/wiki/Landau-Symbole#Definition) (потому что немцы придумали)
Википедии

## Частные случаи (1)

Когда $f$ «меньше» $T$, а именно

$f(n)=O(n^c)$, при этом $c < \log_b a$,

справедливо

$$T(n)=\Theta \left( n^{\log_b a} \right)$$

Примеры:

* Алгоритм Штрассена:
  $T(n)=7 T\left(\frac{n}{2}\right)+O(n^{2})$,
  выполняется за время
  $O(n^{\log_2 7}) \approx O(n^{2{,}81})$
* Алгоритм Карацубы
  $T(n)=3 T(\lceil n/2\rceil )+cn+d = 3 T(\lceil n/2\rceil )+O(n)$,
  выполняется за время
  $T(n)=\Theta (n^{\log _{2}3})$

## Частные случаи (2)

Когда $f$ «сравнимо» с $T$, а именно

$\exists k\geq 0$:
$f(n)=\Theta (n^{c}\log^{k}n)$, где $c=\log_{b}a$,

справедливо

$$T(n)=\Theta(n^{c}\log^{k+1}n)$$

Примеры:

* Двоичный поиск:
  $T(n)=T\left(\frac{n}{2}\right)+O(1)$, выполняется за время $O(\log n)$

* Сортировка слиянием:
  $T(n)=2T\left(\frac{n}{2}\right)+O(n)$, выполняется за время $O(n\log n)$

## Частные случаи (3)

Когда $f$ «больше» $T$, а именно

$f(n)=\Omega(n^c)$, где $c>\log_{b}a$ и
$\exists N$: $af\left(\frac{n}{b}\right)\leq kf(n)$ для некоторой константы $k<1$ и $n>N$,

справедливо

$$T(n)=\Theta \big(f(n)\big)$$

# Упражнения и вопросы

### Вопросы

* Назовите основные показатели эффективности распараллеливания
* Сформулируйте и обоснуйте закон Амдала
* Приведите примеры алгоритмов, допускающих эффективную параллельную и распределённую реализацию
* Что такое жадные и динамические алгоритмы
* Сформулируйте теорему о рекуррентных соотношениях и её частные случаи

### Упражнения

* Найдите алгоритм, который может подходить под третий случай теоремы о рекуррентных соотношениях и условия, при которых это произойдёт
