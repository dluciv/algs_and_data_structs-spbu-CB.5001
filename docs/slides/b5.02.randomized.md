<span id="slides-title" hidden>Рандомизированные алгоритмы</span>

# Что мы обсуждаем

* Рандомизированные алгоритмы
* Неточные и субоптимальные решения
* Рандомизация
* Примеры конкретных алгоритмов и моделей
* Реализация и применение алгоритмов и моделей

= = = = = =

# Рандомизированные алгоритмы

- - - - - -

## Понятие рандомизированного алгоритма

**Рандомизированный алгоритм** — алгоритм, в котором выполнение одного или несколько шагов основано на слу-чайном правиле, т. е. среди многи хдетерминированны хправил од-но выбирается случайно в соответствии с вероятностью

Рандомизированный алгоритм называется **вероятностно-успешным** с вероятностью $p$, если вероятность егоправильного результата не меньше $p$

**Литература**

* Граничин О.Н. [Рандомизированные алгоритмы в задачах обработки данных и принятия решений](https://www.math.spbu.ru/user/gran/papers/10580575.pdf) / О.Н. Граничин // Системное программирование 6(1) 2011 — СПб.: Издательство С.-Петербургского университета - С. 141-162
* [Журнал «Стохастическая оптимизация в информатике»](https://www.elibrary.ru/title_about.asp?id=28931)
* Любой учебник по теории вероятностей, рекомендованный среди литературы к соответствующему курсу

- - - - - -

## Примеры классов рандомизированных алгоритмов

* На основе метода Монте-карло *
* На основе «метода» Лас-Вегаса
* На основе локального рандомизированного (не градиентного) спуска
  * Генетические — подбор параметров, задачи комивояжёра т т.д. *
* Алгоритмы аппроксимации функций
  * Compressive sensing 
* Построение маршрутов *

@pause@

\* Радикальное снижение вычислительной сложности за счёт допустимости *субоптимальных решений*

@pause@

Для [некоторых алгоритмически неразрешимых задач](https://ru.wikipedia.org/wiki/%D0%90%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B8_%D0%BD%D0%B5%D1%80%D0%B0%D0%B7%D1%80%D0%B5%D1%88%D0%B8%D0%BC%D0%B0%D1%8F_%D0%B7%D0%B0%D0%B4%D0%B0%D1%87%D0%B0#%D0%94%D1%80%D1%83%D0%B3%D0%B8%D0%B5_%D0%BF%D1%80%D0%BE%D0%B1%D0%BB%D0%B5%D0%BC%D1%8B) существуют алгоритмы поиска неточных и неполных решений

= = = = = =

# Метод Монте-карло

- - - - - -

## Область применения метода Монте-Карло

Приближённое вычисление интегральных величин, требующих сканирования областей определения. Например (буквально):

$$\int_{x \in \Omega} f(x)dx$$

можно численно вычислить методами прямоугольников или трапеций...

@pause@

... но только когда мощность $\Omega$ сравнительно невелика.

- - - - - -

## Вычисление определённых интегралов методом Монте-Карло

Для $\Omega = \prod_{i=1}^N [x_{i,1}, x_{i,2}]$ и $f: D(f) = \Omega \rightarrow V(f) = [\min_\Omega f, \max_\Omega f]$ возьмём N равномерно распределённых точек $(x_1, \ldots, x_n, y) \in D(f) \times V(f)$; при этом $\exists K \in 0:N:$ для $K$ из этих точек $f(x_1, \ldots, x_n) \ge y$. Тогда

$$\int_{x\in\Omega} f(x)dx \approx \operatorname{MC}_f(\Omega) = |D(f) \times V(f)| \frac{K}{N}$$

- - - - - -

## Кто придумал и развивал метод Монте-Карло

* Джон фон Нейман при работе в Манхеттенском проекте в 1940-х
* 1950-е ... уже много кто и много где
* Много наших коллег в частности под руководством проф. С.М. Ермакова

- - - - - -

# ...

<div style="text-align: center;">

![](img5/05.randomized/monte-carlo_knight.jpg) <!--.element: style="width: 85%;" -->

</div>

- - - - - -

## Пример задачи для метода Монте-Карло

Объём $N$-мерного шара:

$$V_n(R) = \frac{\pi^{n/2}}{\Gamma(\frac{n}{2} + 1)}R^n$$

@pause@

Допустим мы не умеем вычислять $\Gamma$ и не знаем, что в отдельности для чётно- и нечётномерных пространств можно обойтись без неё, зная, что $\Gamma(z+1)=z!$ для $z \in \mathbb{Z}^+$.

- - - - - -

# Немного из теории вероятностей (1)

**Случайная величина** ...

@pause@

**Функция распределения** случайной величины

$$\operatorname{F}_\xi(x) = \operatorname{P}(\xi \le x)$$

@pause@

**Плотность распределения** случайной величины

* Для непрерывных $\operatorname{p}\_\xi(x): \int_{-\infty}^x \operatorname{p}\_\xi(x)dx = \operatorname{F}\_\xi(x) $
* Для дискретных --- аналогично, но сумма

@pause@

**Математическое ожидание**

$$\operatorname{E}(\xi) = \int_{-\infty}^{\infty} x d \operatorname{F}_\xi(x)$$

Для дискретных --- взвешенная сумма

@pause@

**Дисперсия** случайной величины

$$\operatorname{D}(\xi) = \operatorname{E}((\xi - \operatorname{E}\xi)^2)$$

- - - - - -

# Немного из теории вероятностей (2)

**Закон больших чисел** (в формулировке Чебышёва): если $\operatorname{F}\_{\xi\_1} = \ldots = \operatorname{F}\_{\xi\_n}$ и $\operatorname{E}\xi\_{1}^{2} < \infty$, то

$$\frac{1}{n}\sum_{i=1}^{n}\xi_i \rightarrow \operatorname{E}\xi_1$$

**Неравенство Чебышёва**

$$\operatorname{P}(|\xi-\operatorname{E}\xi|\ge\varepsilon) \le \frac{\operatorname{D}\xi}{\varepsilon^2}$$

@pause@

**Центральная предельная теорема**

$\xi_1, \ldots \xi_n$ --- независимые одинаково распределённые случайные величины, такие что $\operatorname{E}\xi_1 = \mu, \operatorname{D}\xi_1 = \sigma^2$. Тогда

$$\frac{\sum_{i=1}^n {\xi_i} - n\mu}{\sigma\sqrt{n}} \rightarrow N(0, 1)$$

где $N(0, 1)$ --- *нормальное распределение*: $\operatorname{p}_{N(0, 1)}(x) = \frac{e^{-1/2 x^2}}{\sqrt{2\pi}}$

- - - - - -

# Точность метода Монте-Карло теоретически

$$\operatorname{P} ( \left| \frac{\int_{x\in\Omega} f(x)dx}{|D(f) \times V(f)|} -
  \frac{K}{N} \right| \ge \varepsilon )$$

  $$\le$$

 $$
  \frac{
    \int_{x\in\Omega} f(x)dx(|D(f) \times V(f)| - \int_{x\in\Omega} f(x)dx)
  }{
    N \varepsilon^2 |D(f) \times V(f)|^2
  }
  $$

- - - - - -

# Точность метода Монте-Карло практически

<div style="text-align: center;">

![Из Википедии](img5/05.randomized/Pi_30K.gif) <!--.element: style="width: 50%;" -->

</div>

- - - - - -

## Оптимизация

Различные способы, предписывающие использовать не равномерное распределение в $\Omega$ и учитывающие это распределение при вычислении окончательного результата.

@pause@

Напрмер, в форме построения: разбить $\Omega$ на параллелотопы $\Omega_j$. При этом, вероятно, уменьшится (по крайней мере не увеличится) $|V_{\Omega_j}(f)| \le |V_\Omega(f)|$, что снизит количество точек.

Фактически, это эвристический гибрид с методом прямоугольников.

= = = = = =

# Метод Лас-Вегаса

- - - - - -

## Суть метода

* Есть критерий проверки правильности решения
* Есть алгоритм, позволяющий генерировать все возможные решения

@pause@

* Повторять алгоритм, пока решение не удовлетворит критерию

- - - - - -

## Пример

Алгоритм сортировки **Bogosort** --- случайно переставлять элементы массива, пока массив не отсортируется.

Смешно?

@pause@

Пожалуй. Пока что. Ещё варианты и шутка про квантовый компьютер:

https://en.wikipedia.org/wiki/Bogosort#Related_algorithms

[Демонстрация Bogosort](https://www.youtube.com/watch?v=CSe0MWDLevA&lc=Ugyb4JdzuDtmYQhVM5p4AaABAg.8y_vesgJKXQ9C8jOrLC91j)

= = = = = =

## Генерация случайных перестановок

Тасование Фишера-Йетса

* Оригинал (для бумаги) — вычёркивать по одному случайному из оставшихся
  * Fisher R. A., Yates F. [Statistical tables: For biological, agricultural and medical research](https://scholar.google.com/scholar?cluster=8866406322625103057). – Oliver and Boyd, 1938.
* Сейчас — Durstenfeld R. [Algorithm 235: random permutation](https://scholar.google.com/scholar?cluster=9651389948038421552) // Communications of the ACM. – 1964. – Т. 7. – №. 7. – С. 420.
  * Не вычёркивать, а переносить в конец — [менять с последним, затем предпоследним, затем предпредпоследним и т.д. элементами](https://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle#The_modern_algorithm)

Одна из проблем — генерация случайных чисел в заданном диапазоне. Durstenfeld в своей статье писал:
$... j := entier(i \times random - 1); ...$ — как будто это так легко. Но мы знаем, что если мы генерируем биты, то при генерации в заданном диапазоне может теряться от доли бита до целых наборов битов.

= = = = = =

# Compressive sensing

- - - - - -

## Теорема Котельникова

* Количество информации и количество данных

@pause@

* Сигнал и несущая

@pause@

**Неформальная формулировка** (если такое возможно) — для восстановления сигнала, состощего из периодических фунций необходима частота дискретизации (и следовательно несущая) вдвое или более превосходящая максимальную частоту в сигнале

- - - - - -

## Применение теоремы Котельникова для регулярного базиса

Регулирование сжатия JPEG — управление размером (полнотой) ортонормированного тригонометрического базиса

- - - - - -

## Принцип, аналогичный теореме Котельникова, для произвольного базиса

Иногда получается превзойти ограничения теоремы Котельникова, в т.ч. и с учётом специфики входных данных:

* Базис может учитывать природу данных и быть в итоге меньше, чем «универсальные» (например, тригонометрический)

- - - - - -

## Пример: сжатие изображения

... Jupyter...

= = = = = =

## Рандомизация для повышения точности

- - - - - -

# Примеры

* Повышение точности физического измерения при помощи рандомизации
  * ... Jupyter...
* «Комфортный шум» в АЦП и в телефонных линиях

= = = = = =

# Поиск путей и заполнение пространства

- - - - - -

## Зачем это нужно?

Для поиска пути между двумя точками через невыпуклую фигуру. Например, в лабиринте.

* [В основном, в робототехнике и в других приложениях, связанных с планированием движения](https://scholar.google.com/scholar?q=rapidly+exploring+random+tree).
* Но можно, например, для построения плотного покрытия и комбинировать с градиентным спуском.
  Если покрытие достаточно плотное, переходить к градиентному спуску.

- - - - - -

## Random Tree

1. Ставим корень
2. Берём случайный узел
3. Пускаем из него побег с новым узлом на конце
4. Повторяем с (2)

@pause@

Хорошо, но медленно

@pause@

Особенно для многомерных пространств. Аналогично случайным блужданиям по решёткам в теории вероятностей: когда измерения два, пересечения почти наверняка есть, когда больше --- очень часто нет.

- - - - - -

## RRT

<div style="text-align: center;">

![](img5/05.randomized/rrtkuffner_small.jpg) <!--.element: style="width: 50%;" -->

</div>

- - - - - -

## RRT: принцип и результат

Принцип — расти в направлении, в котором нет узлов

@pause@

* Берём случайную точку
* Находим ближайшую вершину
* Растём из неё в направлении этой точки

@pause@

http://lavalle.pl/rrt/

* Демонстрация — [раз](https://youtu.be/Ob3BIJkQJEw), [два](https://demonstrations.wolfram.com/RapidlyExploringRandomTreeRRTAndRRT/)
* [Пример реализации](https://gist.github.com/dluciv/cb0c78e7b23e2a4d972669e91227d4f1)

**Литература**

* LaValle S. M., Kuffner Jr J. J. [Randomized kinodynamic planning](https://scholar.google.com/scholar?cluster=9762044935364914725) // The international journal of robotics research. – 2001. – Т. 20. – №. 5. – С. 378-400.

- - - - - -

## Муравьиные алгоритмы

**Муравьиный алгоритм** — рандомизированный алгоритм *роевого интеллекта*, имитирующий коллективный поиск муравьями кратчайшео пути между муравейником и источником пищи с отметкой успешных путей феромонами

@pause@

**Литература**

* Dorigo M., Colorni A., Maniezzo V. [Distributed optimization by ant colonies](https://scholar.google.com/scholar?cluster=5014049766536212931). – 1991.
* Чураков Михаил, Якушев Андрей. [Муравьиные Алгоритмы](https://blog.bullgare.com/wp-content/uploads/2019/05/aca.pdf). ИТМО, 2007

= = = = = =

# Вероятностная проверка простоты

* Тест Ферма
* Тест Миллера-Рабина
* Тест Бейли-Померанца-Селфриджа-Уогстаффа 

- - - - - -

## Зачем и почему?

Задача проверки на простоту имеет [достаточно высокую вычислительную сложность](https://en.wikipedia.org/wiki/Primality_test#Complexity) --- уверенно можно сказать, что сложность полиномиальная (оценка сверху)

@pause@

Хорошо, а зачем проверка больших чисел на простоту?..

@pause@

Чтобы убедиться, что они простые, например, для криптографии:

1. Берём случайное
2. Проверяем на простоту. Вероятностный тест определяет, точно ли оно является составным.
3. Если точно составное, берём ещё одно
4. Если м.б. простое, проверяем точно и медленно

Случаные числа дорогие, простые дорогие, а случайные простые — совсем дорогие!

**Источники**

Популярный, но полезный [CryptoWiki](http://cryptowiki.net/index.php?title=%D0%90%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC%D1%8B,_%D0%B8%D1%81%D0%BF%D0%BE%D0%BB%D1%8C%D0%B7%D1%83%D0%B5%D0%BC%D1%8B%D0%B5_%D0%BF%D1%80%D0%B8_%D1%80%D0%B5%D0%B0%D0%BB%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D0%B8_%D0%B0%D1%81%D0%B8%D0%BC%D0%BC%D0%B5%D1%82%D1%80%D0%B8%D1%87%D0%BD%D1%8B%D1%85_%D0%BA%D1%80%D0%B8%D0%BF%D1%82%D0%BE%D1%81%D1%85%D0%B5%D0%BC#.D0.A2.D0.B5.D1.81.D1.82_.D0.A4.D0.B5.D1.80.D0.BC.D0.B0_.28Fermat_test.29)

- - - - - -

## Тест Ферма

**Малая теорема Ферма** $\forall a, p:$ если $p$ простое, то $a^{p − 1} \equiv 1 \mod p$.

**Тест Ферма**: если для $a > 1\;\; a^{p − 1} \equiv 1 \mod p$, $p$ --- простое или псевдопростое.

**Псевдопростоее число** --- составное число, проходящее тест Ферма.

@pause@

Jupyter

- - - - - -

### Числа Кармайкла

**Число Кармайкла** — составное число $n$, которое удовлетворяет $а^{n-1}\equiv 1\pmod{n}$ для всех целых $а$, взаимно простых $n$

Их достаточно много (вообще бесконечно =)), и тест Ферма на них ошбается при любом количестве итераций.

- - - - - -

## Тест Соловея — Штрассена

Не ошибается на числах Кармайкла

@pause@

Jupyter

- - - - - -

## Тест Миллера-Рабина

- - - - - -

## Тест Бейли-Померанца-Селфриджа-Уогстаффа 

= = = = = =

# Вопросы и упражнения

## Вопросы

* Что такое субоптимальные решения и почему иногда ищут именно их?
* Что такое рандомизированный алгоритм?
* Опишите тасование Фишера-Йетса — оригинальное описание и алгоритм Дюрштенфельда
* Опишите принцип Compressive Sensing
* Сформулируйте теорему Котельникова
* Приведите пример повышения точности измерений при помощи рандомизации
* Приведите примеры способов задачи поиска пути между двумя точками среди препятствий
* Что такое Rapidly Exploring Random Tree?
* Что такое муравьиный алгоритм

## Упражнения

* Самостоятельно изучите дерево RRT\*, расскажите про него
* В примере с Compressive Sensing разбейте изображение на фрагменты 8x8, а случайный базис ещё и ортогонализуйте; посмотрите, какого размера нужен будет базис
* Реализуйте муравьиный алгоритм, смоделируйте его работу для любой задачи
