<span id="slides-title" hidden>Алгоритмы сжатия</span>

# Что мы обсуждаем

* Данные и информация
* Семейства алгоритмов сжатия
* Хронология и история конкретных алгоритмов

= = = = = =

# Данные и информация

- - - - - -

## Информация

* **ГОСТ 7.0-1999**:  Информация — сведения, воспринимаемыечеловекоми (или) специальными устройствами как отражение фактов материального или духовного мира в процессе коммуникации.
* **ФЗ №149 от 27.07.2006** «Об информации, информационных технологиях и о защите информации»: (Ст. 2):  Информация — сведения (сообщения, данные) независимо от формы их представления.

@pause@

Алсо

* **ГОСТ Р 43.0.21-2020** [Информационное обеспечение техники и операторской деятельности. Сознание и самосознание](http://protect.gost.ru/v.aspx?control=8&baseC=-1&page=0&month=-1&year=-1&search&RegNum=1&DocOnPageCount=15&id=228133&pageK=BD6E8F95-C8A6-47DB-AFB1-CD97A760834F)

- - - - - -

## Информация и данные

Пример данных

```
92d0 d020 d0bd d1b0 d087 d0b0 d0bb 20b5
b1d0 8bd1 bbd0 bed0 d020 d0a1 d0bb d0be
d0b2 0abe
```

@pause@

Пример менее избыточных данных (данные $-$ избыточость)

```
20c2 e0ed e0f7 e5eb e120 ebfb 20ee ebd1
e2ee 0aee
```
@pause@

Пример информации (данные $+$ интерпретация)

```В начале было Слово```

- - - - - -

## Что же такое информация

* **Информация** — данные $+$ интерпретация

Избыточность иногда упрощает реализацию интерпретации в общем или частном случае (как в примере выше с UTF-8 vs Windows-1251)

- - - - - -

## Данные и энергия

Рудольф Ландауэр, IBM, 1961: потеря одного бита приводит к выделению теплоты

$$E_\mathrm{bit} > E_\mathrm{SNL} =k_\mathrm{B} \, T \, \ln 2,$$

где

* Константа Больцмана $k_\mathrm{B} \approx 1.38 \cdot 10^{-23}$ Дж/К — связь между температурой и энергией идеального газа
* Потенциальный барьер $E_\mathrm{SNL}$ для изменения состояния электрона при $T = 300 \mathrm{K}$ $~$ $E_\mathrm{SNL} \approx 0.017$ эВ $\approx 2.7×10^{−21}$ Дж

**Литература**

* Bennett C. H. [Notes on Landauer's principle, reversible computation, and Maxwell's Demon](https://www.cs.princeton.edu/courses/archive/fall06/cos576/papers/bennett03.pdf) // Studies In History and Philosophy of Science Part B: Studies In History and Philosophy of Modern Physics. – 2003. – Т. 34. – №. 3. – С. 501-510.
* Menin B. M. [Is There a Relationship between Energy, Amount of Information and Temperature?](https://www.journalpsij.com/index.php/PSIJ/article/view/30148) // Physical Science International Journal. – 2019. – С. 1-9.

- - - - - -

# Энтропия

Упрощённое неформальное определение по мотивам К. Шеннона

* **Энтропия** — количество информации, которой не хватает для полного определения состояния системы.

@pause@

Пример связи с термодинамикой: сжимая газ, мы приводим систему в состояние с повышенной определённостью, и тратим на это энергию

**Источники**

* Популярная статья [Энтропия? Это просто!](https://habr.com/ru/post/374681/)

- - - - - -

# Единицы измерения информационной энтропии

<div style="text-align: center;">

![](img5/03.compression/w_Entropy_flip_2_coins.jpg) <!--.element: style="width: 55%;" -->

</div>

Информационная энтропия измеряется в...

@pause@

Безразмерная. Количество разрядов, длина возможной строки, которую требуется дописать для полного определения состояния. Величина зависит от алфавита ($\Rightarrow$ основания логарифма)

* Бит ($\log_2 \ldots$)
* Трит ($\log_3 \ldots$), равен $\log_2 3 \approx 1.58$ бита
* Хартли ($\lg \ldots$), равен $\log_2 10 \approx 3.32$ бита
* Нат ($\ln \ldots$), равен $\log_2 e \approx 1.44$ бита — вспомним $e$-ичную систему!

- - - - - -

## Связь с количеством данных и информации

Неформальное определение

**Сжатие** — устранение избыточных степеней свободы в математической модели системы

Пояснение: «лишние» биты обеспечивают кажущуюся подвижность, но для большинства текстов от них можно избавиться

= = = = = =

# Оптимизация алфавита

- - - - - -

## Эвристика и изощрённый ум

* ASCII не универсален
* UTF-16 и UTF-32 не экономны
* UTF-8 для значительной доли текстов оптимальна
* Для исключительно локальных текстов ещё оптимальнее локальные однобайтные кодировки

- - - - - -

## Посимвольное и глобальное сжание

Построение алфавита (префиксного кода) Шеннона на основе частотного анализа данных — наиболее частые фрагменты даных должны кодироваться самыми короткими кодами

Алгоритмы:

* Алгоритм Хаффмана
* Алгоритм арифметического кодирования

- - - - - -

## Посимвольное сжатие: Алгоритм Хаффмана

Строит словарь кода Шеннона, оптимальный для случая, когда вероятность встретить символ равна $2^{-n}$

Хаффман, 1952

- - - - - -

## Глобальное сжатие: арифметическое кодирование

Идеальное энтропийное кодирование: если символ встречается с вероятностью $P$, на его кодирование должно расходоваться $\log_2 P$ бит (или $\log_3 P$ трит и т.д.)

Арифметичское кодирование позволяет кодировать символ нецелым количеством бит:

1. Вычислить частоты символов
2. Разбить отрезок $[0, 1)$ на фрагменты, пропорционально вероятностям символов
3. Выбрать фрагмент, соответствующий первому символу
4. Рекуррентно повторить в нём для второго символа, и т.д.
5. Последний символ кодировать любым (например, кратчайшим по записи) числом из его фрагмента

@pause@

Формально операции ведутся над длинными рациональными числами, но время от времмени имеет смысл «прерывать» входной поток, чтобы они не становились слишком уж длинными.

@pause@

А можно не утруждаться, и сделать [вот так](https://web.archive.org/web/20201016142713/https://algor.skyparadise.org/read/3) =).

**Источники**

* Richard Clark Pasco. Source coding algorithms for fast data compression. Ph.D. disssertation, Stanford Univ., 1976

= = = = = =

# Сжатие при помощи поиска повторяющихся фрагментов

- - - - - -

## Run Length Encoding

Хранить значение и то, сколько раз оно повторяется

Несмотря на очевидность, метод эффективен для:

* изображений определённого вида (растровый рендер простой векторной графики)
* рядов данных
* используется в GIF, PCX

@pause@

* Возможен вариант, чем-то напоминающий Хаффмана: длинные последовательности в одном диапазоне будут иметь повторяющиеся старшие разряды

**Источники**

* Robinson A. H., Cherry C. Results of a prototype television bandwidth compression scheme // Proceedings of the IEEE. – 1967. – Т. 55. – №. 3. – С. 356-364

- - - - - -

## Lempel, Ziv, Welch (1977, 1984)

* LZ-77
* LZ-78
* LZW-1984 — первый популярный и широко реализуемый

**Источники**

* Welch T. A. A technique for high-performance data compression // Computer. – 1984. – №. 6. – С. 8-19.
* https://habr.com/ru/post/132683/
* http://www.compression.ru/arctest/descript/comp-hist.htm

- - - - - -

## LZW

https://youtu.be/8uFqfZOiwMc

- - - - - -

## BZip2

Построен на последовательности: преобразование Барроуза-Вилера → RLE → Арифметическое кодирование

**Источники**

* Burrows M., Wheeler D. J. [A block-sorting lossless data compression algorithm](https://www.hpl.hp.com/techreports/Compaq-DEC/SRC-RR-124.html). – Tech. Report, DEC, 1994.

- - - - - -

# Вопросы и упражнения

## Вопросы

* Что такое информационная энтропия?
* Опишите идею кода Шеннона и алгоритма Хаффмана
* Опишите идею арифметического кодирования
* Что такое RLE?
* Опишите идею алгоритма LZW
* Опишите идею преобразования Барроуза-Вилера

## Упражнения

* Перепишите пример на арифметическое кодирование с разбиением текста на блоки во избежание выхода за пределы машинных целых
* Реализуйте сжатие и восстановление LZW
